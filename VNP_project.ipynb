{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOgwdyf91vIL",
    "outputId": "0ba7e371-e656-4d08-db7b-71a8d987241d"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zTwF4289L7p",
    "outputId": "5425f82f-c6a2-466a-905e-450e97fa0243"
   },
   "outputs": [],
   "source": [
    "#Get all links from 2025 onwards every 6 hours - gdelt_latest_export_links_per_day_2025.txt\n",
    "\n",
    "# import re\n",
    "# import requests\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# master_url = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
    "# pattern = re.compile(r'http://data\\.gdeltproject\\.org/gdeltv2/(\\d{14})\\.export\\.CSV\\.zip')\n",
    "# start_date = datetime(2025, 1, 1)\n",
    "# output_file = \"gdelt_latest_export_links_per_6h_2025.txt\"\n",
    "\n",
    "# response = requests.get(master_url)\n",
    "# latest_per_slot = {}\n",
    "\n",
    "# for line in response.text.splitlines():\n",
    "#     match = pattern.search(line)\n",
    "#     if match:\n",
    "#         file_datetime = datetime.strptime(match.group(1), '%Y%m%d%H%M%S')\n",
    "#         if file_datetime >= start_date:\n",
    "#             # Calculate the 6-hour slot for this timestamp\n",
    "#             slot_hour = (file_datetime.hour // 6) * 6\n",
    "#             slot_start = file_datetime.replace(hour=slot_hour, minute=0, second=0)\n",
    "#             slot_key = slot_start.strftime('%Y%m%d%H')\n",
    "#             # If this is the first file for the slot or later than the current, keep it\n",
    "#             if (slot_key not in latest_per_slot) or (file_datetime > latest_per_slot[slot_key][0]):\n",
    "#                 latest_per_slot[slot_key] = (file_datetime, match.group(0))\n",
    "\n",
    "# # Sort by slot and write URLs to output file\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     for slot in sorted(latest_per_slot):\n",
    "#         f.write(latest_per_slot[slot][1] + \"\\n\")\n",
    "\n",
    "# print(f\"Wrote {len(latest_per_slot)} URLs (one per 6-hour slot) to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U2kpzqzAG76i",
    "outputId": "928b335c-4ea0-45e9-9eab-f8814eec3b1f"
   },
   "outputs": [],
   "source": [
    "#Get all links from 2025 onwards every 15 min - gdelt_latest_export_links_per_day_2025.txt\n",
    "\n",
    "import re\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "master_url = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
    "pattern = re.compile(r'http://data\\.gdeltproject\\.org/gdeltv2/(\\d{14})\\.export\\.CSV\\.zip')\n",
    "start_date = datetime(2025, 1, 1)\n",
    "output_file = \"gdelt_latest_export_links_per_15min_2025.txt\"\n",
    "\n",
    "response = requests.get(master_url)\n",
    "latest_per_slot = {}\n",
    "\n",
    "for line in response.text.splitlines():\n",
    "    match = pattern.search(line)\n",
    "    if match:\n",
    "        file_datetime = datetime.strptime(match.group(1), '%Y%m%d%H%M%S')\n",
    "        if file_datetime >= start_date:\n",
    "            # Calculate the 15-minute slot for this timestamp\n",
    "            minute_slot = (file_datetime.minute // 15) * 15\n",
    "            slot_start = file_datetime.replace(minute=minute_slot, second=0)\n",
    "            slot_key = slot_start.strftime('%Y%m%d%H%M')\n",
    "            # Keep the latest file for each slot\n",
    "            if (slot_key not in latest_per_slot) or (file_datetime > latest_per_slot[slot_key][0]):\n",
    "                latest_per_slot[slot_key] = (file_datetime, match.group(0))\n",
    "\n",
    "# Sort by slot and write URLs to output file\n",
    "with open(output_file, \"w\") as f:\n",
    "    for slot in sorted(latest_per_slot):\n",
    "        f.write(latest_per_slot[slot][1] + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(latest_per_slot)} URLs (one per 15-minute slot) to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0HSj2cJnYf-",
    "outputId": "56463046-2492-415d-f656-649573d591d8"
   },
   "outputs": [],
   "source": [
    "#Get all topics related to the US tariffs - us_tariff_results.csv\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import csv\n",
    "import re\n",
    "\n",
    "url_file = 'C://Users//Petarche//Downloads/gdelt_latest_export_links_per_15min_2025.txt'  # Your file with the list of URLs\n",
    "output_file = 'us_tariff_results.csv'\n",
    "\n",
    "# Keywords\n",
    "keywords = [\n",
    "    'trade deal', 'us tariff', 'u.s. tariff', 'tariff', 'tariffs', 'US tariffs' 'united states tariff', 'american tariff',\n",
    "    'trump tariff', 'biden tariff', 'trade war', 'import duty', 'steel tariff', 'china tariff'\n",
    "]\n",
    "keyword_pattern = re.compile('|'.join(keywords), re.IGNORECASE)\n",
    "\n",
    "with open(url_file, 'r') as f:\n",
    "    urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as outcsv:\n",
    "    writer = csv.writer(outcsv)\n",
    "    writer.writerow(['SourceFile', 'GDELT_Row'])  # Header\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Processing: {url}\")\n",
    "        try:\n",
    "            r = requests.get(url, timeout=60)\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            # Find the CSV file inside the zip\n",
    "            csvname = [name for name in z.namelist() if name.endswith('.CSV')][0]\n",
    "            with z.open(csvname) as csvfile:\n",
    "                for row in csvfile:\n",
    "                    try:\n",
    "                        # Decode and search for keywords in the row\n",
    "                        row_str = row.decode('utf-8', errors='ignore')\n",
    "                        if keyword_pattern.search(row_str):\n",
    "                            writer.writerow([url, row_str.strip()])\n",
    "                    except Exception as e:\n",
    "                        continue  # skip problematic lines\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {url}: {e}\")\n",
    "\n",
    "print(f\"Done! Results saved in {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5hd_O6tnkgV",
    "outputId": "2aba9a6a-3a7a-454a-94fa-4323788d64d0"
   },
   "outputs": [],
   "source": [
    "!pip install trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39wsZceCnmc2",
    "outputId": "754d9076-f3f2-40fb-f7cc-276a57751e8a"
   },
   "outputs": [],
   "source": [
    "#Extract the other data from the GDELT_Row from us_tariff_results.csvas separate columns - merged_columns.csv\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "GDELT_COLUMNS = [\n",
    "    \"event_id\", \"event_date\", \"event_month\", \"event_year\", \"event_year_decimal\",\n",
    "    \"actor1_country_code\", \"actor1_country_name\", \"actor1_code\", \"actor1_type\",\n",
    "    \"actor1_code_num\", \"event_code\", \"event_root_code\", \"quad_class\", \"goldstein_scale\",\n",
    "    \"num_mentions\", \"num_sources\", \"num_articles\", \"avg_tone\", \"actor1_geo_type\",\n",
    "    \"actor1_geo_fullname\", \"actor1_geo_country\", \"actor1_geo_adm1\", \"actor1_geo_adm2\",\n",
    "    \"actor1_geo_lat\", \"actor1_geo_long\", \"actor1_geo_feature_id\", \"actor2_geo_type\",\n",
    "    \"actor2_geo_type_dup\", \"actor2_geo_fullname\", \"actor2_geo_country\", \"actor2_geo_adm1\",\n",
    "    \"actor2_geo_adm2\", \"actor2_geo_lat\", \"actor2_geo_long\", \"actor2_geo_feature_id\",\n",
    "    \"event_timestamp\", \"source_url\"\n",
    "]\n",
    "\n",
    "input_csv = 'us_tariff_results.csv'\n",
    "output_csv = 'merged_columns.csv'\n",
    "\n",
    "def parse_gdelt_row(row_str):\n",
    "    # Split on spaces\n",
    "    parts = row_str.strip().split()\n",
    "    if len(parts) < 37:\n",
    "        # Not enough fields, skip\n",
    "        return None\n",
    "    # The last field is always the URL\n",
    "    url = parts[-1]\n",
    "    fields = parts[:-1]\n",
    "    # If there are more than 36 fields, merge extras into the last field before URL (for city names with spaces)\n",
    "    if len(fields) > 36:\n",
    "        fields = fields[:35] + [' '.join(fields[35:])]\n",
    "    return fields + [url]\n",
    "\n",
    "with open(input_csv, newline='', encoding='utf-8') as infile, \\\n",
    "     open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.DictReader(infile, fieldnames=['SourceFile', 'GDELT_Row'])\n",
    "    writer = csv.writer(outfile)\n",
    "    # Write header\n",
    "    writer.writerow(['SourceFile'] + GDELT_COLUMNS)\n",
    "    for row in reader:\n",
    "        sourcefile = row['SourceFile']\n",
    "        gdelt_row = row['GDELT_Row']\n",
    "        fields = parse_gdelt_row(gdelt_row)\n",
    "        if not fields:\n",
    "            continue\n",
    "        writer.writerow([sourcefile] + fields)\n",
    "\n",
    "print(f\"Done! Data saved in {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0IEIYLBRnpTS",
    "outputId": "f3872258-d74d-47b8-f3d5-6b551a913975"
   },
   "outputs": [],
   "source": [
    "#Trafilatura extracts all the news articles in a separate column from the links from merged_columns.csv - extracted_articles.csv\n",
    "\n",
    "import csv\n",
    "import trafilatura\n",
    "\n",
    "input_csv = 'C://Users//Petarche//Downloads/merged_columns.csv'\n",
    "output_csv = 'extracted_texts_all.csv'\n",
    "max_articles = 1000000\n",
    "skip_domains = ['cbc.ca', 'timescolonist.com', 'malaya.com']\n",
    "seen_links = set()\n",
    "\n",
    "def get_article_text(url):\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if downloaded:\n",
    "            text = trafilatura.extract(downloaded)\n",
    "            return text if text else \"\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "with open(input_csv, newline='', encoding='utf-8') as infile, \\\n",
    "     open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    header = next(reader)\n",
    "\n",
    "    # Find required column indices\n",
    "    try:\n",
    "        url_col_idx = header.index('source_url')\n",
    "    except ValueError:\n",
    "        raise Exception(\"No 'source_url' column found in the input file.\")\n",
    "    try:\n",
    "        event_date_col_idx = header.index('event_date')\n",
    "    except ValueError:\n",
    "        raise Exception(\"No 'event_date' column found in the input file.\")\n",
    "\n",
    "    # All other columns except event_date and source_url\n",
    "    other_columns = [col for i, col in enumerate(header) if i not in (event_date_col_idx, url_col_idx)]\n",
    "\n",
    "    # New header order\n",
    "    new_header = ['id', 'event_date', 'source_url', 'text', 'trust_factor'] + other_columns\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(new_header)\n",
    "\n",
    "    article_count = 0\n",
    "    for row in reader:\n",
    "        if article_count >= max_articles:\n",
    "            break\n",
    "        link = row[url_col_idx]\n",
    "        if not (link.startswith(\"http://\") or link.startswith(\"https://\")):\n",
    "            continue\n",
    "        if any(domain in link for domain in skip_domains):\n",
    "            continue\n",
    "        if link in seen_links:\n",
    "            continue\n",
    "        seen_links.add(link)\n",
    "        event_date = row[event_date_col_idx]\n",
    "        other_values = [val for i, val in enumerate(row) if i not in (event_date_col_idx, url_col_idx)]\n",
    "        print(f\"[{article_count+1}] Processing: {link}\")\n",
    "        text = get_article_text(link)\n",
    "        new_row = [article_count+1, event_date, link, text, 1] + other_values\n",
    "        writer.writerow(new_row)\n",
    "        article_count += 1\n",
    "\n",
    "print(f\"Done! Extracted {article_count} articles. Data saved in {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DerD0VoR2OIe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wYbceFBW2PjY",
    "outputId": "24722632-5036-4b28-b106-1632c0d708c1"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"extracted_texts_all.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHdPHO2o2UN5",
    "outputId": "db6b870f-5e89-4f72-b8e9-090957f7ba67"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qeqB09n2Y8f"
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEoYInRW3r_N",
    "outputId": "104d9e8c-fafa-4fb2-8bcd-6c91b3d9ed88"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Qv07F8a3tJM"
   },
   "outputs": [],
   "source": [
    "# We think that event_month, event_year and event_year_decimal* should be dropped just because they are redundant values and we have that data in one column \"event_date\"\n",
    "# also the column \"id\" because is just a sequential number with no meaningful information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GNA_3Wf35aoH",
    "outputId": "483287c5-6f28-4c53-f467-da46dcb59c60"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['id'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2P_nYWFq5264"
   },
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_nan_v1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfbJAcKn6WxO",
    "outputId": "fd6cf844-44f7-440a-cc89-1474705c4bc4"
   },
   "outputs": [],
   "source": [
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ywv_aIVw7X-Q",
    "outputId": "bddf3373-d313-4cb5-de5a-ad125fa1bd40"
   },
   "outputs": [],
   "source": [
    "redundant_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
    "print(f\"Redundant columns: {redundant_cols}\")\n",
    "\n",
    "# The output is trust_factor , because right now all rows with that column\n",
    "# are with values = 1 , which is made by purpose from us , until we fine tune a model, so it can choose the trust factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "xnFwY5677aGT",
    "outputId": "812ed6c8-05c5-499c-e8cb-e935a99cfc4d"
   },
   "outputs": [],
   "source": [
    "# This five ['goldstein_scale', 'num_mentions', 'num_sources', 'num_articles', 'avg_tone'] columns are of type object,\n",
    "# most of them have numerical value, bit still it contains some text, like WORLD,BANK,MNC etc...\n",
    "#\n",
    "\n",
    "#numeric_cols = ['goldstein_scale', 'num_mentions', 'num_sources', 'num_articles', 'avg_tone']\n",
    "#df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "#\n",
    "df[\"num_articles\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "2f27aw6i77Ws",
    "outputId": "16169b88-94f8-4daf-f01a-b1df875f0c75"
   },
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix for Numeric Columns\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTwya33X9Xtx"
   },
   "outputs": [],
   "source": [
    "# As we can see from the correlation heatmap the following columns are pretty highly correlated, >0.7  , they should be dropped ([\"event_year_decimal\",\"event_month\", \"event_year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2lTK0OT94_z",
    "outputId": "f1e395b4-bb15-4f3f-ade4-faeec5ce40f3"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['event_year_decimal','event_month', 'event_year'])\n",
    "\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "oQEZvVYeAFBO",
    "outputId": "c9b642c1-7df9-4852-f3da-95b706528171"
   },
   "outputs": [],
   "source": [
    "df.actor1_geo_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "T8M80uJrAOJW",
    "outputId": "ccd9e299-d15f-432b-aa8b-25a36320a090"
   },
   "outputs": [],
   "source": [
    "df.actor1_country_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQATh4dj-twW"
   },
   "outputs": [],
   "source": [
    "# TO DISCUSS WITH PROFESSOR:\n",
    "\n",
    "#We think the following columns are likely redundant pairs\n",
    "\n",
    "#actor1_country_code & actor1_country_name (we think we should keep one)\n",
    "\n",
    "#actor2_geo_type & actor2_geo_type_dup (we need to see if they are duplicates then we should drop duplicates)\n",
    "\n",
    "#actor1_geo_country & actor1_country_code (geographic redundancy maybe?)\n",
    "\n",
    "#SourceFile (file paths rarely add value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHdGBsuwDPEj",
    "outputId": "301f7c83-668d-4ad7-d547-7722c3d68a9d"
   },
   "outputs": [],
   "source": [
    "# List of columns for each actor\n",
    "actor1_geo_cols = [\n",
    "    'actor1_geo_type', 'actor1_geo_fullname', 'actor1_geo_country',\n",
    "    'actor1_geo_adm1', 'actor1_geo_adm2'\n",
    "]\n",
    "actor2_geo_cols = [\n",
    "    'actor2_geo_type', 'actor2_geo_fullname', 'actor2_geo_country',\n",
    "    'actor2_geo_adm1', 'actor2_geo_adm2'\n",
    "]\n",
    "\n",
    "# Summary statistics\n",
    "print(df[actor1_geo_cols].describe(include='all'))\n",
    "print(df[actor2_geo_cols].describe(include='all'))\n",
    "\n",
    "# Top 10 most frequent values for each column\n",
    "for col in actor1_geo_cols + actor2_geo_cols:\n",
    "    print(f\"\\nTop 10 values for {col}:\")\n",
    "    print(df[col].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Yj-nf86_DSZ7",
    "outputId": "0fbabadc-11db-48ae-9921-7ff32fd4b915"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for col in actor1_geo_cols + actor2_geo_cols:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.countplot(data=df, y=col, order=df[col].value_counts().head(10).index)\n",
    "    plt.title(f\"Top 10 {col} values\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XNhdeppDbcM"
   },
   "outputs": [],
   "source": [
    "# TO DISCUSS WITH PROFESSOR:\n",
    "# Some rows are int and some other rows have strings, because of that we can't come to any conclusions with the data we currently have, which is a bit mixed."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
